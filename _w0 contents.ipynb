{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w1 - summarize content  \n",
    "- summaries text with **openAI** or open source **Ollama** APIs\n",
    "- create a brochure from scraped web pages\n",
    "- create a technical tutor\n",
    "\n",
    "## w2 - Function calls. Create webapp chatbot with multi-modal AI\n",
    "Create webapp and chatbot for airline costumer service that finds the price of a ticket to a city, generates image of the city with the main tourist attractions and speaks the answer back.\n",
    "\n",
    "- day1: Anthropic and google APIs and chat between two AIs\n",
    "- day2: Web interface with Gradio\n",
    "- day3: Cerate chat with gradio \n",
    "- day4: add function calls (tools) to an AI chatbot. Add tool to find the ticket price\n",
    "- day5: Add Agents to AI - multi-modal AI\n",
    "\n",
    "## w3 - open source Gen AI with Hugging Face (HF)\n",
    " - day2: Pipelines\n",
    " - day3: Tokenizers \n",
    " - day4: Use different HF models\n",
    " - day5: Create minutes of meetings. \n",
    " - Synthetic data generator\n",
    "\n",
    " ## w4 - Evaluating models and deploying on Hugging Face\n",
    " - day1-2: useful links:\n",
    "    - [hugging Face open models leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/)\n",
    "    - [Embedding Models Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "    - [HF LLM-Perf Leaderboard ](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)\n",
    "    - [HF coding models leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)\n",
    "    - [HF explore financial analysis spaces](https://huggingface.co/spaces?category=financial-analysis)\n",
    "    - [human ranking](https://lmarena.ai/)\n",
    "    - [vellum leaderboard - includes API cost](https://www.vellum.ai/llm-leaderboard)\n",
    "    - [artificialanalysis leaderboard](https://artificialanalysis.ai/models)\n",
    "    - [task specific models leaderboard](https://scale.com/leaderboard)\n",
    " - day3: Convert python to c++ code and deploy in a gradio app\n",
    " - day4: Deploying models on Hugging Face\n",
    " ## w5 Implementing a RAG pipeline\n",
    " - day1: Brute force RAG - Understand the idea od RAG\n",
    " - day2: load text docs and split in chunks with LangChain\n",
    " - day3: Vectorize some docs - LangChain for vector embeddings \n",
    " - day4: Implement a RAG pipeline. \n",
    " - day5: Optimizing RAG systems \n",
    " ## w6 Fine Tune LLM with Lora/qLora - frontier model  \n",
    " - day1: Get the data from HF check it out and curate it. \n",
    "   - remove text that does not add value - like batteries included \n",
    "   - replace weird chars, and excess white spaces and item parts numbers\n",
    "   - create a prompt\n",
    " - day2: Create clean dataset and model deployment pipeline. \n",
    "   - day2.1: Load multiple data sets in parallel to save time\n",
    "   - day2.2: balance the dataset so that it contains similar sample sizes for different prices and categories. \n",
    "   - day2.3: create trane and test datasets.\n",
    "   - day2.4: LLM Deployment pipeline\n",
    "      1. understand the business problem \n",
    "         - What data is available.\n",
    "         - no fictional requirements - cost constrains, scalability latency \n",
    "         - time to market - implementation timeline\n",
    "         - how to measure - business focused metrics \n",
    "      2. preparation - test baseline models, prep dataset \n",
    "         - research existing solutions \n",
    "         - compare LLMs - context length, price, license, leader board, specialist scores. \n",
    "         - curate the data - clean, preprocess, balance, split. \n",
    "      3. select model/s \n",
    "         - choose LLMs\n",
    "         - experiment with them \n",
    "         train and validate with curated data\n",
    "      4. Optimize the model\n",
    "         - prompting\n",
    "         - RAG\n",
    "         - fine-tunning \n",
    "      5. productionalize the model \n",
    "         - define an API \n",
    "         - identify hosting and deployment architecture \n",
    "         - address scaling, monitoring and security\n",
    "         - measure the business focussed metrics \n",
    "         - continues measure and model improvement - retrain model. \n",
    " - day3: Create a baseline model for comparison\n",
    "## w7 Fine Tune LLM with Lora/qLora - open source model\n",
    "## w8 Build autonomous multi-Agent system\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
